# V1 — Naive Baseline

[← Back to overview](../README.md)

## Problem

We need a simple URL shortener: users POST a long URL and get a short code back; visitors GET the short code and get redirected. The system must be correct, but in V1 we intentionally keep the architecture simple to establish a **measurable baseline** before optimizing.

## Architecture

```
┌────────┐       ┌──────────────────┐       ┌──────────────┐
│ Client │──────▶│  Spring Boot API │──────▶│  PostgreSQL  │
│        │◀──────│  (single inst.)  │◀──────│  (single DB) │
└────────┘       └──────────────────┘       └──────────────┘
```

Every single request hits the database directly. No cache, no replicas, no horizontal scaling.

## API

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/shorten` | Create a short URL |
| GET | `/r/{code}` | Redirect to original URL (302) |

**Create:**

```bash
curl -X POST http://localhost:8080/api/shorten \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com/very-long-path"}'
```

```json
{ "shortUrl": "http://localhost:8080/r/aBcD3fG" }
```

**Redirect:**

```bash
curl -v http://localhost:8080/r/aBcD3fG
# → 302 Found, Location: https://example.com/very-long-path
```

## How to Run

**Prerequisites:** Docker and Docker Compose installed.

```bash
# Start everything (Postgres + App)
docker compose up --build

# App is available at http://localhost:8080
```

To run only Postgres (for local development):

```bash
docker compose up postgres

# Then run the Spring Boot app from your IDE or:
# mvn spring-boot:run
```

To stop and clean up:

```bash
docker compose down -v
```

## Load Testing

### Option A — Docker (recommended, no local install needed)

```bash
# With the stack running (docker compose up --build), execute:
docker compose run --rm k6 run baseline.js
```

Custom parameters:

```bash
docker compose run --rm k6 run -e SEED_COUNT=1000 baseline.js
```

> The container's `working_dir` is `/scripts`, so you only need the filename.

### Option B — Local k6

**Prerequisites:** [k6](https://grafana.com/docs/k6/latest/set-up/install-k6/) installed.

```bash
# With the app running, execute:
k6 run load-test/baseline.js
```

Custom parameters:

```bash
k6 run -e BASE_URL=http://localhost:8080 -e SEED_COUNT=1000 load-test/baseline.js
```

---

The baseline test seeds 500 URLs, then ramps from 10 to 200 virtual users over ~2.5 minutes with a 90/10 read/write mix.

### Stress Test (find the breaking point)

```bash
docker compose run --rm k6 run stress.js
```

The stress test seeds 1000 URLs and ramps through 6 stages up to 500 VUs (~3 min total). Tuned for local Docker environments where all containers share the same CPU.

| Stage | Duration | VUs | Goal |
|-------|----------|-----|------|
| 1 — Warm-up | 20 s | 10 → 100 | Light load, confirm health |
| 2 — Baseline | 30 s | 100 → 200 | Match baseline test |
| 3 — Moderate | 30 s | 200 → 300 | 1.5x baseline, detect early degradation |
| 4 — Heavy | 30 s | 300 → 400 | 2x baseline, DB connection pressure |
| 5 — Stress peak | 30 s + 20 s hold | 400 → 500 | 2.5x baseline, expect degradation |
| 6 — Cool-down | 20 s | 500 → 0 | Ramp down, observe recovery |

## Results

### Baseline (200 VUs, 500 seed URLs, 90/10 read/write)

| Metric | Value |
|--------|-------|
| Throughput (req/s) | 1,125 |
| Latency avg | 1.73 ms |
| Latency p90 | 2.84 ms |
| Latency p95 | 4.08 ms |
| Latency max | 189.69 ms |
| Error rate | 0.00% |
| Total requests | 170,991 |
| Reads (GET /r/) | 152,844 |
| Writes (POST /api/shorten) | 17,147 |
| Test duration | ~2 min 32 s |

> At 200 VUs the system handles the load comfortably — every request hits PostgreSQL directly, yet latency stays under 5 ms at p95. This is the number to beat in future versions.

### Stress Test (500 VUs, 1000 seed URLs, 90/10 read/write)

| Metric | Value |
|--------|-------|
| Throughput (req/s) | 2,790 |
| Latency avg | 2.1 ms |
| Latency p90 | 3.1 ms |
| Latency p95 | 4.7 ms |
| Latency max | 208.2 ms |
| Error rate | 0.00% |
| Total requests | 508,365 |
| Test duration | ~3 min 02 s |

### Near-Critical (700 VUs, 1000 seed URLs, 90/10 read/write)

| Metric | Value |
|--------|-------|
| Throughput (req/s) | 6,331 |
| Latency avg | 4.2 ms |
| Latency p90 | 8.6 ms |
| Latency p95 | 14.3 ms |
| Latency max | 213.3 ms |
| Error rate | 0.00% |
| Total requests | 899,611 |
| Test duration | ~2 min 22 s |

### Comparison

| Metric | Baseline (200 VUs) | Stress (500 VUs) | Near-Critical (700 VUs) |
|--------|---------------------|-------------------|--------------------------|
| Throughput | 1,125 req/s | 2,790 req/s | 6,331 req/s |
| Latency avg | 1.73 ms | 2.1 ms | 4.2 ms |
| Latency p90 | 2.84 ms | 3.1 ms | 8.6 ms |
| Latency p95 | 4.08 ms | 4.7 ms | 14.3 ms |
| Latency max | 189.69 ms | 208.2 ms | 213.3 ms |
| Error rate | 0.00% | 0.00% | 0.00% |

> **Takeaway:** The system survived all three tests with zero errors, but the trend is clear. At 700 VUs the p95 latency jumped to **14.3 ms** — a **3.5x increase** over the baseline's 4.08 ms — while throughput grew to 6,331 req/s. The latency curve is no longer linear: from 200→500 VUs it barely moved (+15%), but from 500→700 VUs it **tripled** (+204%). This non-linear degradation is the early sign of saturation — the single PostgreSQL instance is approaching its connection and CPU limits. Adding Redis (V3) would absorb the ~90% read traffic and flatten this curve dramatically.

## Where It Hurts

This baseline architecture has intentional bottlenecks that the next versions will address:

| Pain Point | When It Surfaces | Fixed In |
|---|---|---|
| **Single point of failure** | App crash = 100% downtime | V2 (stateless + LB) |
| **Every GET hits the DB** | High read traffic = high latency | V3 (Redis cache) |
| **Single DB instance** | Write throughput ceiling | V4 (sharding) |
| **No read replicas** | Can't scale reads independently | V5 (replication) |

## What's Next: V2

Remove application state, put multiple instances behind a load balancer, and prove the system survives partial failures.

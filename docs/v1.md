# V1 — Naive Baseline

[← Back to overview](../README.md)

## Problem

We need a simple URL shortener: users POST a long URL and get a short code back; visitors GET the short code and get redirected. The system must be correct, but in V1 we intentionally keep the architecture simple to establish a **measurable baseline** before optimizing.

## Architecture

```text
┌────────┐       ┌──────────────────┐       ┌──────────────┐
│ Client │──────▶│  Spring Boot API │──────▶│  PostgreSQL  │
│        │◀──────│  (single inst.)  │◀──────│  (single DB) │
└────────┘       └──────────────────┘       └──────────────┘
```

Every single request hits the database directly. No cache, no replicas, no horizontal scaling.

## API

| Method | Endpoint       | Description                    |
|--------|----------------|--------------------------------|
| POST   | `/api/shorten` | Create a short URL             |
| GET    | `/r/{code}`    | Redirect to original URL (302) |

**Create:**

```bash
curl -X POST http://localhost:8080/api/shorten \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com/very-long-path"}'
```

```json
{ "shortUrl": "http://localhost:8080/r/aBcD3fG" }
```

**Redirect:**

```bash
curl -v http://localhost:8080/r/aBcD3fG
# → 302 Found, Location: https://example.com/very-long-path
```

## How to Run

**Prerequisites:** Docker and Docker Compose installed.

```bash
# Start everything (Postgres + App)
docker compose up --build

# App is available at http://localhost:8080
```

To run only Postgres (for local development):

```bash
docker compose up postgres

# Then run the Spring Boot app from your IDE or:
# mvn spring-boot:run
```

To stop and clean up:

```bash
docker compose down -v
```

## Load Testing

### Option A — Docker (recommended, no local install needed)

```bash
# With the stack running (docker compose up --build), execute:
docker compose run --rm k6 run baseline.js
```

Custom parameters:

```bash
docker compose run --rm k6 run -e SEED_COUNT=1000 baseline.js
```

### Option B — Local k6

**Prerequisites:** [k6](https://grafana.com/docs/k6/latest/set-up/install-k6/) installed.

```bash
# With the app running, execute:
k6 run load-test/baseline.js
```

Custom parameters:

```bash
k6 run -e BASE_URL=http://localhost:8080 -e SEED_COUNT=1000 load-test/baseline.js
```

---

The baseline test seeds 500 URLs, then ramps from 10 to 200 virtual users over ~2.5 minutes with a 90/10 read/write mix.

### Stress Test

```bash
docker compose run --rm k6 run stress.js
```

Ramps through 6 stages up to 700 VUs (~2.5 min). Measures latency degradation under growing load.

### Breakpoint Test (find the real ceiling)

```bash
docker compose run --rm k6 run breakpoint.js
```

Ramps linearly to 2000 VUs until the system starts failing. This is where we find the actual limit of a single Postgres + single app instance.

| Stage        | Duration | VUs          | Goal                  |
|--------------|----------|--------------|-----------------------|
| 1 — Ramp     | 30 s     | 10 → 500     | Past known safe zone  |
| 2 — Push     | 30 s     | 500 → 1000   | Double baseline peak  |
| 3 — Beyond   | 30 s     | 1000 → 1500  | Unknown territory     |
| 4 — Break    | 30 s     | 1500 → 2000  | Likely breaking point |
| 5 — Hold     | 20 s     | 2000         | Confirm failures      |
| 6 — Cooldown | 20 s     | 2000 → 0     | Observe recovery      |

### Chaos Test (prove single point of failure)

```bash
# Terminal 1 — start the chaos test
docker compose run --rm k6 run chaos.js

# Terminal 2 — while k6 is running, kill the app
docker compose stop app

# Watch k6 output: error rate spikes to 100%
# Then restart:
docker compose start app
```

Steady 100 VUs for 60 seconds. The goal is not performance — it's to prove that **one container down = total outage**.

## Results

![V1 Load Test Charts](/docs/v1-charts.png)

### Baseline (200 VUs)

| Metric             | Value       |
|--------------------|-------------|
| Throughput (req/s) | 1,125       |
| Latency p95        | 4.08 ms     |
| Error rate         | 0.00%       |
| Total requests     | 170,991     |

### Stress (700 VUs)

| Metric             | Value       |
|--------------------|-------------|
| Throughput (req/s) | 6,331       |
| Latency p95        | 14.3 ms     |
| Error rate         | 0.00%       |
| Total requests     | 899,611     |

### Breakpoint (up to 2000 VUs)

| Metric             | Value         |
|--------------------|---------------|
| Throughput (req/s) | 9,131         |
| Latency avg        | 62.4 ms       |
| Latency p90        | 133.0 ms      |
| Latency p95        | 159.2 ms      |
| Latency max        | 493.9 ms      |
| Error rate         | 0.00%         |
| Total requests     | 1,505,154     |

> The system survived 2000 VUs with zero errors — but latency tells the real story. p95 went from **4.08 ms** (baseline) to **159.2 ms** — a **39x increase**. The max latency hit 493.9 ms, dangerously close to timeout territory. The single instance didn't break, but it's completely saturated: adding more VUs would only increase latency further with no throughput gain. And there's still no way to add capacity without downtime.

### Chaos (kill app mid-traffic)

| Metric           | Value              |
|------------------|--------------------|
| Total requests   | 3,018              |
| Successes        | 2,000              |
| Failures         | 518                |
| Error rate       | 17.16%             |
| Throughput       | 47.5 req/s         |
| Duration         | ~63 s              |

> App was killed mid-test with `docker compose stop app`. During downtime, **100% of requests failed** — every single one. The 17% overall error rate reflects the ~10 seconds the app was down out of the 60s test. There is no failover, no second instance, no recovery. One process dies, all users get errors.

### Comparison

| Metric       | Baseline (200 VUs) | Stress (700 VUs) | Breakpoint (2000 VUs) | Chaos              |
|--------------|--------------------|-----------------:|----------------------:|:------------------:|
| Throughput   | 1,125 req/s        |    6,331 req/s   |           9,131 req/s | 47.5 req/s         |
| Latency p95  | 4.08 ms            |       14.3 ms    |             159.2 ms  | —                  |
| Error rate   | 0.00%              |         0.00%    |                0.00%  | 17.16%             |
| Availability | 100%               |           100%   |                 100%  | **0%** during kill |

## Where It Hurts

The baseline and stress tests show the system handles traffic well — latency grows but never breaks. That's the **throughput story**, but it hides the two real problems:

**1. There is a ceiling, and past it everything fails at once.**
The breakpoint test finds the exact VU count where errors start. There's no graceful degradation — the single instance either works or it doesn't. You can't add capacity without downtime.

**2. One container down = total outage.**
The chaos test proves it. Kill the app container, and 100% of requests fail instantly. There's no failover, no recovery, no second instance to pick up traffic. A deploy, a crash, an OOM kill — anything that stops the process means every user gets an error.

| Problem                           | Evidence              | Fixed In            |
|-----------------------------------|-----------------------|---------------------|
| **No horizontal scaling**         | Breakpoint test       | V2 (stateless + LB) |
| **Single point of failure**       | Chaos test            | V2 (stateless + LB) |
| **Every GET hits the DB**         | Stress p95 trend      | V3 (Redis cache)    |
| **Single DB, no write scale-out** | Breakpoint ceiling    | V4 (sharding)       |
| **No read replicas**              | All reads hit primary | V5 (replication)    |

## What's Next: V2

The chaos test proved it: **kill one instance and 100% of traffic dies**. V2 solves this by making the app stateless, putting multiple instances behind a load balancer, and proving the system survives partial failures — you kill one, the others keep serving.
